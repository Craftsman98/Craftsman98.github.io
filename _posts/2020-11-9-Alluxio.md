
---
tags: Alluxio Kubernetes
---


Alluxio 是世界上第一个面向基于云的数据分析和人工智能的开源的[数据编排技术](https://www.alluxio.io/blog/data-orchestration-the-missing-piece-in-the-data-world/)。 它为数据驱动型应用和存储系统构建了桥梁, 将数据从存储层移动到距离数据驱动型应用更近的位置从而能够更容易被访问。 这还使得应用程序能够通过一个公共接口连接到许多存储系统。 Alluxio内存至上的层次化架构使得数据的访问速度能比现有方案快几个数量级。

在大数据生态系统中，Alluxio 位于数据驱动框架或应用（如 Apache Spark、Presto、Tensorflow、Apache HBase、Apache Hive 或 Apache Flink）和各种持久化存储系统（如 Amazon S3、Google Cloud Storage、OpenStack Swift、HDFS、GlusterFS、IBM Cleversafe、EMC ECS、Ceph、NFS 、Minio和 Alibaba OSS）之间。 Alluxio 统一了存储在这些不同存储系统中的数据，为其上层数据驱动型应用提供统一的客户端 API 和全局命名空间。

![Ecosystem](/assets/images/Alluxio.assets/alluxio-overview-r050119.svg)

## Alluxio的优势

通过简化应用程序访问其数据的方式（无论数据是什么格式或位置），Alluxio 能够帮助克服从数据中提取信息所面临的困难。Alluxio 的优势包括：

- **内存速度 I/O**：Alluxio 能够用作分布式共享缓存服务，这样与 Alluxio 通信的计算应用程序可以透明地缓存频繁访问的数据（尤其是从远程位置），以提供内存级 I/O 吞吐率。此外，Alluxio的层次化存储机制能够充分利用内存、固态硬盘或者磁盘，降低具有弹性扩张特性的数据驱动型应用的成本开销。
- **简化云存储和对象存储接入**：与传统文件系统相比，云存储系统和对象存储系统使用不同的语义，这些语义对性能的影响也不同于传统文件系统。在云存储和对象存储系统上进行常见的文件系统操作（如列出目录和重命名）通常会导致显著的性能开销。当访问云存储中的数据时，应用程序没有节点级数据本地性或跨应用程序缓存。将 Alluxio 与云存储或对象存储一起部署可以缓解这些问题，因为这样将从 Alluxio 中检索读取数据，而不是从底层云存储或对象存储中检索读取。
- **简化数据管理**：Alluxio 提供对多数据源的单点访问。除了连接不同类型的数据源之外，Alluxio 还允许用户同时连接同一存储系统的不同版本，如多个版本的 HDFS，并且无需复杂的系统配置和管理。
- **应用程序部署简易**：Alluxio 管理应用程序和文件或对象存储之间的通信，将应用程序的数据访问请求转换为底层存储接口的请求。Alluxio 与 Hadoop 生态系统兼容，现有的数据分析应用程序，如 Spark 和 MapReduce 程序，无需更改任何代码就能在 Alluxio 上运行。

### 技术创新点

Alluxio 将三个关键领域的创新结合在一起，提供了一套独特的功能。

1. **全局命名空间**：Alluxio 能够对多个独立存储系统提供单点访问，无论这些存储系统的物理位置在何处。这提供了所有数据源的统一视图和应用程序的标准接口。有关详细信息，请参阅[统一命名空间文档](https://docs.alluxio.io/os/user/stable/cn/core-services/Unified-Namespace.html)。
2. **智能多层级缓存**：Alluxio 集群能够充当底层存储系统中数据的读写缓存。可配置自动优化数据放置策略，以实现跨内存和磁盘（SSD/HDD）的性能和可靠性。缓存对用户是透明的，使用缓冲来保持与持久存储的一致性。有关详细信息，请参阅 [缓存功能文档](https://docs.alluxio.io/os/user/stable/cn/core-services/Caching.html)。
3. **服务器端 API 翻译转换**：Alluxio支持工业界场景的API接口，例如HDFS API, S3 API, FUSE API, REST API。它能够透明地从标准客户端接口转换到任何存储接口。Alluxio 负责管理应用程序和文件或对象存储之间的通信，从而消除了对复杂系统进行配置和管理的需求。文件数据可以看起来像对象数据，反之亦然。

## 在Kubernetes上部署Alluxio

> 先决条件：Kubernetes版本 ≥ 1.8

## Alluxio存储

Alluxio在帮助统一跨各种平台用户数据的同时还有助于为用户提升总体I / O吞吐量。 Alluxio是通过把存储分为两个不同的类别来实现这一目标的。

- **UFS(底层文件存储，也称为底层存储)**  该存储空间代表不受Alluxio管理的空间。 UFS存储可能来自外部文件系统，包括如HDFS或S3。 Alluxio可能连接到一个或多个UFS并在一个命名空间中统一呈现这类底层存储。 -通常，UFS存储旨在相当长一段时间持久存储大量数据。
- Alluxio存储
  - Alluxio做为一个分布式缓存来管理Alluxio workers本地存储，包括内存。这个在用户应用程序与各种底层存储之间的快速数据层带来的是显著提高的I / O性能。
  - Alluxio存储主要用于存储热数据，暂态数据，而不是长期持久数据存储。
  - 每个Alluxio节点要管理的存储量和类型由用户配置决定。
  - 即使数据当前不在Alluxio存储中，通过Alluxio连接的UFS中的文件仍然 对Alluxio客户可见。当客户端尝试读取仅可从UFS获得的文件时数据将被复制到Alluxio存储中。

### 单层存储

在启动时，Alluxio将在每个worker节点上发放一个ramdisk并占用一定比例的系统的总内存。 此ramdisk将用作分配给每个Alluxio worker的唯一存储介质。

```properties
alluxio.worker.ramdisk.size=16GB

# 指定多个存储介质作为单层存储
alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk,/mnt/ssd1,/mnt/ssd2
alluxio.worker.tieredstore.level0.dirs.mediumtype=MEM,SSD,SSD
alluxio.worker.tieredstore.level0.dirs.quota=16GB,100GB,100GB
```

### 多层存储

Alluxio假定根据按I/O性能从高到低来对多层存储进行排序。 例如，用户经常指定以下层：

- MEM(内存)
- SSD(固态存储)
- HDD(硬盘存储)

#### 写数据

用户写新的数据块时，默认情况下会将其写入顶层存储。如果顶层没有足够的可用空间， 则会尝试下一层存储。如果在所有层上均未找到存储空间，因Alluxio的设计是易失性存储，Alluxio会释放空间来存储新写入的数据块。 根据块注释策略，空间释放操作会从work中释放数据块。 [块注释策略](https://docs.alluxio.io/os/user/stable/cn/core-services/Caching.html#block-annotation-policies)。 如果空间释放操作无法释放新空间，则写数据将失败。 

#### 读数据

如果数据已经存在于Alluxio中，则客户端将简单地从已存储的数据块读取数据。 如果将Alluxio配置为多层，则不一定是从顶层读取数据块， 因为数据可能已经透明地挪到更低的存储层。

用`ReadType.CACHE_PROMOTE`读取数据将在从worker读取数据前尝试首先将数据块挪到 顶层存储。也可以将其用作为一种数据管理策略 明确地将热数据移动到更高层存储读取。

#### 配置示例

```properties
# 配置两层存储
alluxio.worker.tieredstore.levels=2
# 将内存存储作为顶层存储
alluxio.worker.tieredstore.level0.alias=MEM
# 将'/mnt/ramdisk'设置为顶层存储的路径
alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk
# 设置ramdisk路径的中间层类型为MEM
alluxio.worker.tieredstore.level0.dirs.mediumtype=MEM
# 设置ramdisk配额使用100GB
alluxio.worker.tieredstore.level0.dirs.quota=100GB
# 配置第二层存储为HDD
alluxio.worker.tieredstore.level1.alias=HDD
# 为第二层存储配置三个单独的路径
alluxio.worker.tieredstore.level1.dirs.path=/mnt/hdd1,/mnt/hdd2,/mnt/hdd3
# 设置HDD为第二层存储的中间类型
alluxio.worker.tieredstore.level1.dirs.mediumtype=HDD,HDD,HDD
# 定义每个文件路径为第二层存储的配额
alluxio.worker.tieredstore.level1.dirs.quota=2TB,5TB,500GB
```

可配置层数没有限制 但是每个层都必须使用唯一的别名进行标识。 典型的配置将具有三层，分别是内存，SSD和HDD。

#### 块注释策略

与写操作同步发生的释放空间操作将尝试根据块注释策略强制顺序删除块并释放其空间给写操作。注释顺序的最后一个块是第一个释放空间候选对象，无论它位于哪个层上。 

 开箱即用的注释策略实施包括: 

- **LRUAnnotator**:根据最近最少使用的顺序对块进行注释和释放。 **这是Alluxio的默认注释策略**。
- **LRFUAnnotator**:根据配置权重的最近最少使用和最不频繁使用的顺序对块进行注释。
  - 如果权重完全偏设为最近最少使用，则行为将 与LRUAnnotator相同。
  - 适用的配置属性包括`alluxio.worker.block.annotator.lrfu.step.factor` `alluxio.worker.block.annotator.lrfu.attenuation.factor`。

### Alluxio数据生命周期管理

- **free**:释放数据是指从Alluxio缓存中删除数据，而不是从底层UFS中删除数据。 释放操作后，数据仍然可供用户使用，但对Alluxio释放文件后尝试访问该文件 的客户端来讲性能可能会降低。
- **load**:加载数据意味着将其从UFS复制到Alluxio缓存中。如果Alluxio使用 基于内存的存储，加载后用户可能会看到I/O性能的提高。
- **persist**:持久数据是指将Alluxio存储中可能被修改过或未被修改过的数据写回UFS。 通过将数据写回到UFS，可以保证如果Alluxio节点发生故障数据还是可恢复的。
- **TTL(Time to Live)**:TTL属性设置文件和目录的生存时间，以 在数据超过其生存时间时将它们从Alluxio空间中删除。还可以配置 TTL来删除存储在UFS中的相应数据。

```shell
# 从alluxio中删除数据
./bin/alluxio fs free ${PATH_TO_UNUSED_DATA}

#将数据加载到Alluxio中
./bin/alluxio fs load ${PATH_TO_FILE}

#在Alluxio中持久化保留数据
./bin/alluxio fs persist ${PATH_TO_FILE}
```

### 在Alluxio中管理数据复制

#### 被动复制

与许多分布式文件系统一样，Alluxio中的每个文件都包含一个或多个分布在集群中存储的存储块。默认情况下，Alluxio可以根据工作负载和存储容量自动调整不同块的复制级别。例如，当更多的客户以类型`CACHE`或`CACHE_PROMOTE`请求来读取此块时Alluxio可能会创建此特定块更多副本。当较少使用现有副本时，Alluxio可能会删除一些不常用现有副本 来为经常访问的数据征回空间([块注释策略](https://docs.alluxio.io/os/user/stable/cn/core-services/Caching.html#block-annotation-policies))。 在同一文件中不同的块可能根据访问频率不同而具有不同数量副本。

默认情况下，此复制或征回决定以及相应的数据传输 对访问存储在Alluxio中数据的用户和应用程序完全透明。

#### 主动复制

除了动态复制调整之外，Alluxio还提供API和命令行 界面供用户明确设置文件的复制级别目标范围。 尤其是，用户可以在Alluxio中为文件配置以下两个属性: 

1. `alluxio.user.file.replication.min`是此文件的最小副本数。 默认值为0，即在默认情况下，Alluxio可能会在文件变冷后从Alluxio管理空间完全删除该文件。 通过将此属性设置为正整数，Alluxio 将定期检查此文件中所有块的复制级别。当某些块 的复制数不足时，Alluxio不会删除这些块中的任何一个，而是主动创建更多 副本以恢复其复制级别。
2. `alluxio.user.file.replication.max`是最大副本数。一旦文件该属性 设置为正整数，Alluxio将检查复制级别并删除多余的 副本。将此属性设置为-1为不设上限(默认情况)，设置为0以防止 在Alluxio中存储此文件的任何数据。注意，`alluxio.user.file.replication.max`的值 必须不少于`alluxio.user.file.replication.min`。

### 检查Alluxio缓存容量和使用情况

```shell
#  Alluxio shell命令`fsadmin report`提供可用空间的简短摘要 以及其他有用的信息 
$ ./bin/alluxio fsadmin report
Alluxio cluster summary:
    Master Address: localhost/127.0.0.1:19998
    Web Port: 19999
    Rpc Port: 19998
    Started: 09-28-2018 12:52:09:486
    Uptime: 0 day(s), 0 hour(s), 0 minute(s), and 26 second(s)
    Version: 2.0.0
    Safe Mode: true
    Zookeeper Enabled: false
    Live Workers: 1
    Lost Workers: 0
    Total Capacity: 10.67GB
        Tier: MEM  Size: 10.67GB
    Used Capacity: 0B
        Tier: MEM  Size: 0B
    Free Capacity: 10.67GB
    
# 查看Alluxio缓存总使用字节数
$ ./bin/alluxio fs getUsedBytes
# 查看Alluxio缓存总容量（以字节为单位）
$ ./bin/alluxio fs getCapacityBytes
```

## Alluxio架构

Alluxio是大数据和机器学习生态系统中的一个新的数据访问层。它位于任何持久化存储系统（如Amazon S3，Microsoft Azure，Apache HDFS或OpenStack Swift）和计算框架（如Apache Spark，Presto或Hadoop MapReduce）之间。Alluxio本身并不是持久性存储系统。

Alluxio作为数据访问层有以下优点：

* 对于用户应用程序和各种计算框架，无论使用的是哪种计算引擎，Alluxio都可以提供快速存储，促进应用间的数据共享。因此，Alluxio可以以内存速度（当数据位于本地）或计算集群网络的速度（当数据位于Alluxio）为数据计算提供服务。首次访问时，仅从底层存储系统读取一次数据，当访问底层存储的速度很慢时，Alluxio就极大的加速了数据访问的速度。为了获得最佳的性能，建议将Alluxio与集群的计算框架一起部署。
* 对于底层存储系统，Alluxio弥合了大数据应用程序与传统存储系统之间的鸿沟，从而扩大了使用数据的工作负荷。由于Alluxio对应用程序隐藏了底层存储系统的集成，因此底层存储系统都以支持在Alluxio上运行的任何应用程序和框架。当同时安装多个存储系统时，Alluxio可以充当多个不同数据源的统一层。

![Architecture overview](/assets/images/Alluxio.assets/architecture-overview-simple-docs.png)

Alluxio可以分为三个部分：**Masters**、**Workers**和**Clients**。一个典型的集群由一个主Master，数个备用Master，一个Job Master，数个备用Job Master，数个Worker和数个Job Worker组成。 主Master和Workers构成了Alluxio Server。Clients用于通过Spark、MapReduce、Alluxio CLI 或 Alluxio FUSE 层进行通信。

Alluxio Job Masters 和 Job Workers 可以被看作一个单独的功能，称为 Job Service。它是一个轻量级的任务计划框架，负责为 Job Workers分配各种类型的工作。包括以下的：

* 从 UFS 加载数据到 Alluxio
* 将数据持久化到 UFS
* 在 Alluxio 内复制文件
* 在 UFS 或 Alluxio 之间移动或复制数据

Job Service 使得与 Job 相关的所有流程不一定都必须与Alluxio集群的其余部分一起放置。但是建议将  Job Worker与相应的 Alluxio Worker 放置在同一个地点，因为它为RPC和数据传输提供了较低的延迟。

### Masters

![Alluxio masters](/assets/images/Alluxio.assets/architecture-master-docs.png)

Alluxio 包括两种不同类型的 master 进程。Alluxio Master服务于所有用户请求和日志文件系统元数据的更改。Alluxio Job Master 充当了即将在 Job Worker上执行的文件系统操作的轻量级调度程序。Alluxio Master 可以部署为一个主Master和几个备用Master以实现高可用。

#### 主Master

Alluxio集群中只能有一个Master作为主Master。主Master负责管理系统的全局元数据（例如innode文件树），块数据和Worker容量的元数据（free和used空间）。主Master只会在底层存储查询元数据。应用数据永远不会通过主服务器进行路由。Alluxio Clients与主Master交互来读取或修改元数据。所有的Worker都会定期的将心跳发送给主Master，以保持在集群中的连接。主Master不会启动与其他组件的通信，它仅通过RPC服务器响应请求。主Master将所有文件系统事务记录到分布式持久性存储位置，以允许恢复主机状态信息（这个记录就是Journal）。

#### 备用Master

备用Master在不同的服务器上启动，以在HA模式下运行Alluxio时，提供容错功能，备用主机读取主要主机的Journal，以使自己的主机状态副本保持最新。备用Master还会编写日记检查点，以在将来能够更快的恢复。但是它们不会处理来自其他Alluxio组件的任何请求。

#### 副Master（适用于UFS Journal）

当运行不具有HA模式的单个Alluxio主服务器并且使用UFS Journal 时，可以在主服务器相同的服务器上启动副服务器来写入日志检查点。副Master并不是为了提供高可用性而设计的，而是将工作从主Master上转移下来，以实现快速的恢复。副Master永远无法升级成为主Master。

#### Job Master

Alluxio Job Master是一个独立的进程，负责在Alluxio中异步调度许多重量级的文件系统操作。通过区分主Master在同一个进程中执行这些操作的需求，Alluxio Master可以使用更少的资源并能够在更短的时间内为更多客户提供服务。

### Workers

![Alluxio workers](/assets/images/Alluxio.assets/architecture-worker-docs.png)



#### Workers

Alluxio workers 负责管理分配给Alluxio的用户可配置的本地资源（例如内存，SSD，HDD）。Alluxio worker将数据存储为块，并通过在其本地资源中读取或创建新块来服务于读取或写入数据的客户端请求。Worker仅负责管理块，从文件到块的实际映射由Master存储。

Worker在底层存储上执行数据操作，这带来了两个重要的优点：

* 从底层存储中读取的数据可以存储在Worker中，并可以立即供其他客户端使用。
* 客户端可以是轻量级的，并且不依赖于底层存储的连接器。

由于RAM通常提供有限的容量，所以当空间已满的时候，可以清除Worker中的块。Worker采用驱逐策略来决定将哪些数据保留在Alluxio空间中。[了解更多](https://docs.alluxio.io/os/user/stable/en/core-services/Caching.html#multiple-tier-storage)

#### Job Workers

Alluxio Job Worker是Alluxio文件系统的客户端。它们负责运行Alluxio Job Master给他们的任务。 Job Workers接收有关在任何给定的文件系统上运行负载、持久化、复制、移动等操作的指令。

Alluxio Job Worker不是必须与Worker位于同一机器，但是推荐将它们放置在同一个物理节点上。

### Client

Alluxio客户端为用户提供了一个与Alluxio服务器进行交互的网关。它启动与主Master的通信来执行队元cp conf/alluxio-site.properties.template conf/alluxio-site.properties数据的操作，并与Worker通信来读取和写入存储在Alluxio中的数据。Alluxio支持Java原生的文件系统API，并且支持多种语言，包括REST、Go 和 Python。Alluxio 同样支持与 HDFS API 和 Amazon S3 API 兼容的API。

### 数据流

本小节描述了基于典型 Alluxio 配置的常见读取和写入情况。Alluxio与计算框架和应用程序部署在一起，持久性存储系统是远程存储集群或基于云的存储。

#### 读数据流

##### Local Cache Hit（本地缓存命中）

Local Cache Hit发生在请求的数据驻留在本地的Alluxio Worker上时。比如，如果应用程序请求通过Alluxio Client，Client会向主Master 请求数据存放在哪个Worker上。如果数据在本地可用，那么Alluxio客户端使用“短路”（short-circuit）规则来绕过Worker直接通过本地文件系统读取数据。短路读取可以避免通过TCP套接字传输数据，并且直接提供数据访问权限，短路读取是从Alluxio读取数据最快的办法。

短路读取不仅可以从内存中读取数据，也可以读取SSD、HDD中的数据，[详细](https://docs.alluxio.io/os/user/stable/en/core-services/Caching.html#multiple-tier-storage)

![Data Flow of Read from a Local Worker](/assets/images/Alluxio.assets/dataflow-local-cache-hit.gif)

##### Remote Cache Hit(远程缓存命中)

当请求的数据存储在Alluxio，而不是存储在Client本地的Worker时，Client将从具有该数据的Worker中远程读取数据。当Client读取完数据之后，Client会指示本地Worker（如果有的话）在本地创建副本，以便将来可以在本地提供对相同数据的读取。Remote Cache Hit 可以提供网络速度的数据读取。由于Alluxio Worker之间的网络速度通常快于Alluxio Worker与底层存储之间的速度，因此Alluxio优先考虑从远程Worker中读取数据。

![Data Flow of Read from a Remote Worker](/assets/images/Alluxio.assets/dataflow-remote-cache-hit.gif)

##### Cache Miss（缓存未命中）

如果数据在Alluxio空间内不可用，则发生Cache Miss，应用程序必须从底层存储中读取数据。Alluxio Client 将来自UFS的读取委派给Worker，最好是本地的Worker。这个Worker从底层存储读取并缓存数据。Cache Miss通常会导致最大的延迟，因为必须从底层存储中获取数据。

当Client仅读取一个块的一部分或非顺序读取该块时，Client将指示Worker异步缓存整个块。异步缓存不会阻塞Client，但是仍然可能会影响性能。可以通过在Worker上设置` alluxio.worker.network.async.cache.manager.threads.max `来调整异步缓存的影响（默认为8）。

![Cache Miss data flow](/assets/images/Alluxio.assets/dataflow-cache-miss.gif)

##### Cache Skip（缓存跳过）

通过将Client中的` alluxio.user.file.readtype.default`设置为`NO_CACHE`来关闭Alluxio的缓存

#### 写数据流

通过设置`alluxio.user.file.writetype.default`来修改Alluxio的写入类型。

##### Write to Alluxio only (`MUST_CACHE`)

写入类型为`MUST_CACHE`时，Alluxio客户端仅写入本地Alluxio Worker，并且不会将任何数据写入底层存储。在写入过程中，如果可以进行短路写入，则为了避免网络传输，Client将绕过Worker直接写入本地RAM。由于数据不会持久存储在底层存储中，因此如果服务器崩溃或需要释放数据以进行新的写入操作时，数据可能会丢失。当允许数据丢失时，`MUST_CACHE`设置对于写入临时数据很有用。

![MUST_CACHE data flow](/assets/images/Alluxio.assets/dataflow-must-cache-1604396107605.gif)

##### Write through to UFS (`CACHE_THROUGH`)

写入类型为`CACHE_THROUGH`时，数据被同步写入到Worker和底层存储中。Client将写操作委托给本地的Worker，这个Worker将数据同时写到本地内存和底层存储中。由于底层存储的写入速度通常比内存慢，Client的写入速度将与底层存储的写入速度相同。

当需要数据持久性时，建议使用`CACHE_THROUGH`类型。

![CACHE_THROUGH data flow](/assets/images/Alluxio.assets/dataflow-cache-through.gif)

##### Write back to UFS (`ASYNC_THROUGH`)

写入类型为`ASYNC_THROUGH`时，数据首先被同步写入到Worker，并在后台持久化到底层存储。`ASYNC_THROUGH`可以以接近`MUST_CACHE`的速度提供数据写入，同时仍然能够持久化数据。

从Alluxio 2.0开始，`ASYNC_THROUGH`时默认的写入类型。

![ASYNC_THROUGH data flow](/assets/images/Alluxio.assets/dataflow-async-through.gif)

为了提供容错能力，与`ASYNC_THROUGH`一起使用的一个重要属性是` alluxio.user.file.replication.durable `。这个属性在写入完成后，数据持久化存储到底层存储之前，在Alluxio中设置新数据的目标复制级别，默认值为1。Alluxio将在后台持久存储过程完成前维护文件的目标复制级别，并且之后再Alluxio中回收空间，因此数据将只被写入UFS一次。

##### Write to UFS Only (`THROUGH`)

写入类型为`THROUGH`时，数据被同步写入到底层存储中而无需缓存到Alluxio Worker。这种写入类型可以确保再写入完成之后将保留数据，但是速度受到底层存储的限制。

##### Data consistency（数据一致性）

 不管写入类型是哪一种，Alluxio空间中的文件/目录始终保持高度一致，因为所有这些写入操作将首先通过Alluxio Master并修改Alluxio文件系统，然后再将其成功返回到客户端/应用软件。因此，只要相应的写入操作成功完成，不同的Alluxio客户端将始终看到最新的更新。

但是，对于依赖数据在UFS中状态的用户或应用程序，不同的写入类型之间可能不同：

* `MUST_CACHE` 数据写入到UFS，则Alluxio空间永远与UFS不一致。
* `CACHE_THROUGH` 数据同步写入到Alluxio和UFS，之后成功返回应用程序
  * 如果对UFS的写入也高度一致（如HDFS），则如果UFS中没有其他的外部更新，则Alluxio空间将始终与UFS一致。
  * 如果写入UFS最终保持一致（如S3），则可能文件已成功写入Alluxio，但是稍后会显示在UFS中。这种情况下，Alluxio客户端仍将看到一致的文件系统，因为它们将始终咨询高度一致的Alluxio Master。因此，尽管不同的Alluxio客户端仍然在Alluxio空间中看到一致的状态，但在数据最终传播到UFS之前可能存在不一致的窗口。
* `ASYNC_THROUGH` 数据写入到Alluxio并返回应用程序，Alluxio将数据异步传播到UFS。从用户角度来看，文件可以成功写入到Alluxio，但以后会保留在UFS中。
* `THROUGH` 数据直接写入UFS，而无需在Alluxio中缓存数据。但是Alluxio知道文件及其状态。因此，元数据仍然是一致的。